{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0d557e5",
   "metadata": {},
   "source": [
    "<h5>Importing libraries</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b4462fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "from PIL import Image\n",
    "\n",
    "# Machine learning (non deep learning)\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "\n",
    "# Deep learning\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# misc\n",
    "import time # for recording model training time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1625e2fa",
   "metadata": {},
   "source": [
    "<h5>Read in images into numpy arrays and record type of tumor with one hot encoder</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d892ca2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lengths of the tumors files\n",
      "length of meningioma tumor files 855\n",
      "length of glioma tumor files 860\n",
      "length of pitulitary tumor files 831\n",
      "length of non-tumor files 454\n",
      "total files is 3000\n"
     ]
    }
   ],
   "source": [
    "img_reshape_size = (32,32)\n",
    "\n",
    "data = []\n",
    "result = []\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "encoder.fit([[0], [1], [2], [3]])  # 0 = no tumor, 1 = meningioma tumor, 2 = glioma tumor , 3 = pitulitary tumor\n",
    "\n",
    "# path to non tumor files\n",
    "no_tumor_image_dir = \"dataset/split_data/multi-class/no_tumor\"\n",
    "no_tumor_files = os.listdir(no_tumor_image_dir)\n",
    "\n",
    "# path to meningioma files\n",
    "meningioma_image_dir = \"dataset/split_data/multi-class/meningioma_tumor\"\n",
    "meningioma_files = os.listdir(meningioma_image_dir)\n",
    "\n",
    "# path to glioma files\n",
    "glioma_image_dir = \"dataset/split_data/multi-class/glioma_tumor\"\n",
    "glioma_files = os.listdir(glioma_image_dir)\n",
    "\n",
    "# path to pitulitary files\n",
    "pitulitary_image_dir = \"dataset/split_data/multi-class/pitulitary_tumor\"\n",
    "pitulitary_files = os.listdir(pitulitary_image_dir)\n",
    "\n",
    "\n",
    "print(\"lengths of the tumors files\")\n",
    "print(\"length of meningioma tumor files \" + str(len(meningioma_files)))\n",
    "print(\"length of glioma tumor files \" + str(len(glioma_files)))\n",
    "print(\"length of pitulitary tumor files \" + str(len(pitulitary_files)))\n",
    "print(\"length of non-tumor files \" + str(len(no_tumor_files)))\n",
    "print(\"total files is \" + str(len(meningioma_files)+len(glioma_files)+len(pitulitary_files)+len(no_tumor_files)))\n",
    "# should be 3000 files in total\n",
    "\n",
    "\n",
    "# could be looped to reduce amount of code\n",
    "\n",
    "## No tumor ##\n",
    "for file in no_tumor_files:\n",
    "    temp_file_path = no_tumor_image_dir + \"/\" +file\n",
    "    img = Image.open(temp_file_path).convert('L')\n",
    "    img = img.resize(img_reshape_size)\n",
    "    img = np.array(img)\n",
    "    data.append(np.array(img))\n",
    "    result.append(encoder.transform([[0]]).toarray())\n",
    "\n",
    "## Meningioma tumor ## \n",
    "for file in meningioma_files:\n",
    "    temp_file_path = meningioma_image_dir + \"/\" + file\n",
    "    img = Image.open(temp_file_path).convert('L')\n",
    "    img = img.resize(img_reshape_size)\n",
    "    img = np.array(img)\n",
    "    data.append(np.array(img))\n",
    "    result.append(encoder.transform([[1]]).toarray())\n",
    "       \n",
    "## Glioma tumor ##\n",
    "for file in glioma_files:\n",
    "    temp_file_path = glioma_image_dir + \"/\"  +file\n",
    "    img = Image.open(temp_file_path).convert('L')\n",
    "    img = img.resize(img_reshape_size)\n",
    "    img = np.array(img)\n",
    "    data.append(np.array(img))\n",
    "    result.append(encoder.transform([[2]]).toarray())\n",
    "        \n",
    "## Pitulitary tumor ##     \n",
    "for file in pitulitary_files:\n",
    "    temp_file_path = pitulitary_image_dir + \"/\"  +file\n",
    "    img = Image.open(temp_file_path).convert('L')\n",
    "    img = img.resize(img_reshape_size)\n",
    "    img = np.array(img)\n",
    "    data.append(np.array(img))\n",
    "    result.append(encoder.transform([[3]]).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6d6cb4",
   "metadata": {},
   "source": [
    "<h5>Reshape data and split between test and train data</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1bf006ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data)\n",
    "result = np.array(result)\n",
    "result = result.reshape(3000,4)\n",
    "x_train,x_test,y_train,y_test = train_test_split(data, result, test_size=0.2, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d094e8",
   "metadata": {},
   "source": [
    "<h5>Build the CNN model</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7abb2af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(2, 2), input_shape=(32, 32, 1), padding = 'Same'))\n",
    "model.add(Conv2D(32, kernel_size=(2, 2),  activation ='relu', padding = 'Same'))\n",
    "\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size = (2,2), activation ='relu', padding = 'Same'))\n",
    "model.add(Conv2D(64, kernel_size = (2,2), activation ='relu', padding = 'Same'))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer='Adamax',  metrics = ['accuracy'])\n",
    "#print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adc402a",
   "metadata": {},
   "source": [
    "<h5>Train the model and record time to train</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c9ecb8cd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "1200/1200 [==============================] - 13s 10ms/step - loss: 1.6731 - accuracy: 0.5525 - val_loss: 1.0509 - val_accuracy: 0.5867\n",
      "Epoch 2/300\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.9201 - accuracy: 0.6358 - val_loss: 0.8052 - val_accuracy: 0.6383\n",
      "Epoch 3/300\n",
      "1200/1200 [==============================] - 11s 9ms/step - loss: 0.8114 - accuracy: 0.6754 - val_loss: 0.7525 - val_accuracy: 0.6683\n",
      "Epoch 4/300\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.6956 - accuracy: 0.7208 - val_loss: 0.6199 - val_accuracy: 0.7533\n",
      "Epoch 5/300\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.6195 - accuracy: 0.7387 - val_loss: 0.6997 - val_accuracy: 0.7283\n",
      "Epoch 6/300\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.5561 - accuracy: 0.7767 - val_loss: 0.6278 - val_accuracy: 0.7483\n",
      "Epoch 7/300\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.4837 - accuracy: 0.8138 - val_loss: 0.5787 - val_accuracy: 0.7933\n",
      "Epoch 8/300\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.4349 - accuracy: 0.8308 - val_loss: 0.5299 - val_accuracy: 0.7983\n",
      "Epoch 9/300\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.3812 - accuracy: 0.8621 - val_loss: 0.5062 - val_accuracy: 0.8183 loss:\n",
      "Epoch 10/300\n",
      "1200/1200 [==============================] - 11s 10ms/step - loss: 0.3649 - accuracy: 0.8629 - val_loss: 0.6977 - val_accuracy: 0.7717\n",
      "Epoch 11/300\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.2968 - accuracy: 0.8867 - val_loss: 0.3884 - val_accuracy: 0.8667969 - accu\n",
      "Epoch 12/300\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.2701 - accuracy: 0.9021 - val_loss: 0.4285 - val_accuracy: 0.8683\n",
      "Epoch 13/300\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.2662 - accuracy: 0.8971 - val_loss: 0.4090 - val_accuracy: 0.8683\n",
      "Epoch 14/300\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.2154 - accuracy: 0.9237 - val_loss: 0.4450 - val_accuracy: 0.8717\n",
      "Epoch 15/300\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.2123 - accuracy: 0.9212 - val_loss: 0.3836 - val_accuracy: 0.8783\n",
      "Epoch 16/300\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.1977 - accuracy: 0.9246 - val_loss: 0.4062 - val_accuracy: 0.8967\n",
      "Epoch 17/300\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.1627 - accuracy: 0.9388 - val_loss: 0.3471 - val_accuracy: 0.8983\n",
      "Epoch 18/300\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.1431 - accuracy: 0.9450 - val_loss: 0.3948 - val_accuracy: 0.8983\n",
      "Epoch 19/300\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.1445 - accuracy: 0.9517 - val_loss: 0.3340 - val_accuracy: 0.9100\n",
      "Epoch 20/300\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.1365 - accuracy: 0.9504 - val_loss: 0.3674 - val_accuracy: 0.9117\n",
      "Epoch 21/300\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.1199 - accuracy: 0.9567 - val_loss: 0.3655 - val_accuracy: 0.9183\n",
      "Epoch 22/300\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.1168 - accuracy: 0.9542 - val_loss: 0.3889 - val_accuracy: 0.9100\n",
      "Epoch 23/300\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.1118 - accuracy: 0.9633 - val_loss: 0.3912 - val_accuracy: 0.9100\n",
      "Epoch 24/300\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.0766 - accuracy: 0.9712 - val_loss: 0.3611 - val_accuracy: 0.9267\n",
      "Epoch 25/300\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.0768 - accuracy: 0.9742 - val_loss: 0.4186 - val_accuracy: 0.9033\n",
      "Epoch 26/300\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.1021 - accuracy: 0.9646 - val_loss: 0.3673 - val_accuracy: 0.9167\n",
      "Epoch 27/300\n",
      "1200/1200 [==============================] - 12s 10ms/step - loss: 0.0719 - accuracy: 0.9721 - val_loss: 0.3818 - val_accuracy: 0.9250\n",
      "--- 317.83943605422974 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "history = model.fit(x_train, y_train, epochs = 300, batch_size = 2, verbose=1, validation_data = (x_test, y_test), callbacks=[callback])\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be35524a",
   "metadata": {},
   "source": [
    "<h6>Some basic results recording, just for keeping information to hand</h6>\n",
    "\n",
    "94.34445095062256 seconds --- 30 epochs , 0.9033 acc , 32x32 RGB image (images are still in greyscale, just RGB format)\n",
    "\n",
    "343.3586163520813 seconds --- 30 epcohs , 0.9182 acc , 64x64 RGB image (images are still in greyscale, just RGB format)\n",
    "\n",
    "1364.8851137161255 seconds --- 30 epochs , 0.8950 acc , 128x128 Greyscale image\n",
    "\n",
    "313.81851744651794 seconds --- 30 epochs , 0.8750 acc , 64x64 Greyscale image\n",
    "\n",
    "87.09897923469543 seconds --- 30 epochs , 0.8833 acc , 32x32 Greyscale image\n",
    "\n",
    "31.2696533203125 seconds --- 30 epochs , 0.8200 acc , 16x16 Greyscale image\n",
    "\n",
    "183.15469479560852 seconds --- 60 epochs , 0.9117 acc , 32x32 Greyscale image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0861b9",
   "metadata": {},
   "source": [
    "<h5>Read in the validation data in the same way we read in the training and testing data</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7eda74d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_reshape_size = (32,32)\n",
    "\n",
    "validation_data = []\n",
    "validation_result = []\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "encoder.fit([[0], [1], [2], [3]]) \n",
    "\n",
    "# path to non tumor files\n",
    "no_tumor_image_dir = \"test_dataset/test/split_data/multi-class/no_tumor\"\n",
    "no_tumor_files = os.listdir(no_tumor_image_dir)\n",
    "\n",
    "# path to meningioma files\n",
    "meningioma_image_dir = \"test_dataset/test/split_data/multi-class/meningioma_tumor\"\n",
    "meningioma_files = os.listdir(meningioma_image_dir)\n",
    "\n",
    "# path to glioma files\n",
    "glioma_image_dir = \"test_dataset/test/split_data/multi-class/glioma_tumor\"\n",
    "glioma_files = os.listdir(glioma_image_dir)\n",
    "\n",
    "# path to pitulitary files\n",
    "pitulitary_image_dir = \"test_dataset/test/split_data/multi-class/pitulitary_tumor\"\n",
    "pitulitary_files = os.listdir(pitulitary_image_dir)\n",
    "\n",
    "\n",
    "for file in no_tumor_files:\n",
    "    temp_file_path = no_tumor_image_dir + \"/\" +file\n",
    "    img = Image.open(temp_file_path).convert('L')\n",
    "    img = img.resize(img_reshape_size)\n",
    "    img = np.array(img)\n",
    "    validation_data.append(np.array(img))\n",
    "    validation_result.append(encoder.transform([[0]]).toarray())\n",
    "\n",
    "for file in meningioma_files:\n",
    "    temp_file_path = meningioma_image_dir + \"/\" + file\n",
    "    img = Image.open(temp_file_path).convert('L')\n",
    "    img = img.resize(img_reshape_size)\n",
    "    img = np.array(img)\n",
    "    validation_data.append(np.array(img))\n",
    "    validation_result.append(encoder.transform([[1]]).toarray())\n",
    "        \n",
    "for file in glioma_files:\n",
    "    temp_file_path = glioma_image_dir + \"/\"  +file\n",
    "    img = Image.open(temp_file_path).convert('L')\n",
    "    img = img.resize(img_reshape_size)\n",
    "    img = np.array(img)\n",
    "    validation_data.append(np.array(img))\n",
    "    validation_result.append(encoder.transform([[2]]).toarray())\n",
    "        \n",
    "        \n",
    "for file in pitulitary_files:\n",
    "    temp_file_path = pitulitary_image_dir + \"/\"  +file\n",
    "    img = Image.open(temp_file_path).convert('L')\n",
    "    img = img.resize(img_reshape_size)\n",
    "    img = np.array(img)\n",
    "    validation_data.append(np.array(img))\n",
    "    validation_result.append(encoder.transform([[3]]).toarray())\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6f488b",
   "metadata": {},
   "source": [
    "<h5>reshape validation data</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "27f62d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = np.array(validation_data)\n",
    "validation_result = np.array(validation_result)\n",
    "validation_result = validation_result.reshape(200,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c6b9d3",
   "metadata": {},
   "source": [
    "<h5>Get and print scores</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c9a1f57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(validation_data, validation_result, verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ec753bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.25100213289260864\n",
      "test accuracy: 0.9350000023841858\n"
     ]
    }
   ],
   "source": [
    "print(\"test loss: \" + str(scores[0]))\n",
    "print(\"test accuracy: \" + str(scores[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb8cb6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
